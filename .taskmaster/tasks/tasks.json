{
  "master": {
    "tasks": [
      {
        "id": 1,
        "title": "Enhance Data Input and Validation",
        "description": "Extend existing CSV loading to support Excel files and implement comprehensive data validation and cleaning",
        "details": "Enhance the existing data loading functionality to support Excel (.xlsx) files using pandas.read_excel(). Implement data validation for: duplicate feature_ids (remove or flag), out-of-range clock positions (0-360 degrees), negative depth values, missing required fields. Add graceful handling of optional fields with default values. Use pandas for robust data type validation and cleaning. Example: df = pd.read_excel(file) if file.endswith('.xlsx') else pd.read_csv(file); df = df.drop_duplicates(subset=['feature_id']); df = df[df['depth_percent'] >= 0]",
        "testStrategy": "Unit tests with malformed CSV/Excel files, duplicate IDs, negative values, missing columns. Verify graceful error handling and data cleaning results.",
        "priority": "high",
        "dependencies": [],
        "status": "done",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Excel file format support",
            "description": "Extend existing CSV loading functionality to support Excel (.xlsx) files using pandas.read_excel()",
            "dependencies": [],
            "details": "Modify the data loading function to detect file extensions and use appropriate pandas method. Implement file type detection: df = pd.read_excel(file) if file.endswith('.xlsx') else pd.read_csv(file). Add error handling for corrupted Excel files and unsupported formats.",
            "status": "done",
            "testStrategy": "Unit tests with valid Excel files, corrupted files, and mixed file types. Verify proper pandas method selection and error handling.",
            "parentId": "undefined",
            "updatedAt": "2026-02-07T20:28:23.093Z"
          },
          {
            "id": 2,
            "title": "Implement duplicate feature_id validation and removal",
            "description": "Add validation to detect and handle duplicate feature_ids in the dataset",
            "dependencies": [
              1
            ],
            "details": "Use pandas drop_duplicates method to remove duplicate entries based on feature_id column. Implement logging to track removed duplicates: df = df.drop_duplicates(subset=['feature_id'], keep='first'). Add option to flag duplicates instead of removing them.",
            "status": "done",
            "testStrategy": "Test with datasets containing duplicate feature_ids. Verify correct removal/flagging and logging of duplicate entries.",
            "parentId": "undefined",
            "updatedAt": "2026-02-07T20:29:21.798Z"
          },
          {
            "id": 3,
            "title": "Implement data range validation for clock positions and depth values",
            "description": "Add validation for clock positions (0-360 degrees) and negative depth values",
            "dependencies": [
              2
            ],
            "details": "Implement range checks: filter clock positions to 0-360 degrees, remove negative depth values using df = df[df['depth_percent'] >= 0]. Add validation for clock position bounds: df = df[(df['clock_position'] >= 0) & (df['clock_position'] <= 360)]. Log invalid entries before removal.",
            "status": "done",
            "testStrategy": "Test with out-of-range clock positions, negative depths, and boundary values. Verify proper filtering and logging of invalid data.",
            "parentId": "undefined",
            "updatedAt": "2026-02-07T20:29:23.855Z"
          },
          {
            "id": 4,
            "title": "Implement missing required fields validation",
            "description": "Add validation to check for missing required fields and handle them appropriately",
            "dependencies": [
              3
            ],
            "details": "Define required fields list and check for missing values using pandas.isnull(). Implement validation: required_fields = ['feature_id', 'depth_percent', 'clock_position']; df.dropna(subset=required_fields). Add detailed error reporting for missing data with row numbers and field names.",
            "status": "done",
            "testStrategy": "Test with datasets missing required fields. Verify proper error reporting and data filtering for incomplete records.",
            "parentId": "undefined",
            "updatedAt": "2026-02-07T20:29:25.515Z"
          },
          {
            "id": 5,
            "title": "Implement graceful handling of optional fields with default values",
            "description": "Add support for optional fields with configurable default values and robust data type validation",
            "dependencies": [
              4
            ],
            "details": "Use pandas fillna() for optional fields with defaults. Implement data type validation using pandas.to_numeric() and pandas.to_datetime() with error handling. Example: df['optional_field'] = df['optional_field'].fillna(default_value); df['numeric_field'] = pd.to_numeric(df['numeric_field'], errors='coerce'). Add configuration for default values.",
            "status": "done",
            "testStrategy": "Test with missing optional fields and invalid data types. Verify default value assignment and type conversion with error handling.",
            "parentId": "undefined",
            "updatedAt": "2026-02-07T20:29:27.041Z"
          }
        ],
        "updatedAt": "2026-02-07T20:29:27.041Z"
      },
      {
        "id": 2,
        "title": "Implement Multi-Point Distance Alignment",
        "description": "Replace constant offset alignment with multi-point alignment using multiple reference features and piecewise linear interpolation within the new modular codebase structure",
        "status": "done",
        "dependencies": [
          "1"
        ],
        "priority": "high",
        "details": "Extend beyond single weld reference to use multiple reference points (welds, valves, fittings). Implement piecewise linear interpolation using scipy.interpolate.interp1d between reference points to handle pipeline stretch/compression. Calculate stretch factors between segments. Support user-provided reference mapping file (CSV with run1_distance, run2_distance, reference_type). Integrate with new modular structure placing alignment logic in src/alignment.py. Work with canonical schema including: run_id, feature_id, distance, odometer, joint_number, relative_position, clock_position_raw, clock_deg, feature_type_raw, feature_type_norm, orientation, depth_percent, length, width, wall_thickness. Example: from scipy.interpolate import interp1d; alignment_func = interp1d(run1_refs, run2_refs, kind='linear', fill_value='extrapolate')",
        "testStrategy": "Test with synthetic data having known stretch factors. Verify interpolation accuracy between reference points. Test edge cases with single reference point falling back to constant offset. Validate integration with modular codebase structure and canonical schema.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create src/alignment.py Module with Reference Point Data Structure",
            "description": "Create the alignment module within the new modular structure and implement reference point data structures with CSV parser",
            "dependencies": [],
            "details": "Create src/alignment.py module as part of the new modular codebase structure. Define ReferencePoint class with distance, reference_type, and metadata fields compatible with canonical schema. Implement CSV parser to read user-provided reference mapping files with columns: run1_distance, run2_distance, reference_type. Add validation for required columns and data types. Handle missing values and duplicate entries with appropriate error messages. Ensure compatibility with canonical schema fields including distance and odometer.",
            "status": "pending",
            "testStrategy": "Test CSV parsing with valid and invalid files. Verify data structure creation and validation logic with edge cases like missing columns or malformed data. Test integration with canonical schema.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Piecewise Linear Interpolation Function in Alignment Module",
            "description": "Create interpolation function using scipy.interpolate.interp1d for distance alignment between reference points within src/alignment.py",
            "dependencies": [
              1
            ],
            "details": "Implement piecewise linear interpolation function in src/alignment.py using scipy.interpolate.interp1d to create alignment function from reference points. Implement linear interpolation with extrapolation support for points outside reference range. Handle edge cases with single reference point by falling back to constant offset. Add error handling for insufficient reference points or invalid data ranges. Ensure proper handling of canonical schema distance and odometer fields.",
            "status": "pending",
            "testStrategy": "Test interpolation accuracy with known reference points. Verify extrapolation behavior at pipeline boundaries. Test fallback to constant offset with single reference point. Validate with canonical schema data.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Calculate Stretch Factors Between Pipeline Segments",
            "description": "Implement stretch factor calculation between consecutive reference points to quantify pipeline deformation",
            "dependencies": [
              2
            ],
            "details": "Calculate stretch factors as ratio of distance differences between consecutive reference points in run1 vs run2. Store stretch factors per segment for analysis and reporting. Handle segments with compression (stretch < 1.0) and expansion (stretch > 1.0). Add validation to detect unrealistic stretch factors that may indicate data errors. Integrate with canonical schema distance fields and log mapping decisions as required.",
            "status": "pending",
            "testStrategy": "Test stretch factor calculations with synthetic data having known deformation. Verify detection of compression and expansion segments. Test validation logic for unrealistic stretch values.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Multi-Point Alignment with src/matching.py Module",
            "description": "Replace existing constant offset alignment with multi-point interpolation in the feature matching pipeline within the modular structure",
            "dependencies": [
              3
            ],
            "details": "Modify src/matching.py to use the multi-point alignment from src/alignment.py instead of constant offset. Apply distance transformation to all features before matching using canonical schema fields. Ensure backward compatibility by supporting both alignment methods through configuration. Update distance calculations in matching algorithms to use aligned distances. Coordinate with src/preprocess.py for data preparation and auto-detection column mapping.",
            "status": "pending",
            "testStrategy": "Compare matching results between constant offset and multi-point alignment methods. Test with real pipeline data to verify improved alignment accuracy. Validate backward compatibility with existing configurations and modular integration.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Configuration and Error Handling for Multi-Point Alignment",
            "description": "Implement configuration options and comprehensive error handling for the multi-point alignment system within the modular architecture",
            "dependencies": [
              4
            ],
            "details": "Add configuration parameters for interpolation method, extrapolation behavior, and stretch factor validation thresholds. Implement comprehensive error handling for missing reference files, invalid data formats, and interpolation failures. Add logging for alignment process steps, mapping decisions (as required by auto-detection), and warnings for potential data quality issues. Create fallback mechanisms when multi-point alignment fails. Ensure proper integration with run_pipeline.py entry point and other modules.",
            "status": "pending",
            "testStrategy": "Test configuration parameter effects on alignment behavior. Verify error handling with various failure scenarios. Test fallback mechanisms and logging output quality. Validate integration with modular codebase and entry point.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Refactor Existing ili_alignment.py Code into Modular Structure",
            "description": "Extract and refactor existing alignment functionality from ili_alignment.py into the new src/alignment.py module",
            "dependencies": [
              5
            ],
            "details": "Identify and extract existing alignment-related code from ili_alignment.py and refactor it into src/alignment.py following the new modular structure. Ensure compatibility with canonical schema fields and maintain existing functionality while integrating with new multi-point alignment features. Update imports and dependencies across the codebase to use the new modular structure. Coordinate with auto-detection column mapping functionality.",
            "status": "pending",
            "testStrategy": "Test that existing alignment functionality continues to work after refactoring. Verify proper integration with new multi-point alignment features. Test compatibility with canonical schema and auto-detection mapping.",
            "parentId": "undefined"
          }
        ],
        "updatedAt": "2026-02-07T20:50:50.835Z"
      },
      {
        "id": 3,
        "title": "Implement Hungarian Algorithm for Optimal Matching",
        "description": "Implement control point identification and matching using Hungarian algorithm for optimal assignment. Identify control points per run (girth welds preferred, plus valves/tees/bends if present in feature_type). Use joint_number matching when available, otherwise use ordered sequence with type and spacing constraints.",
        "status": "pending",
        "dependencies": [
          "2"
        ],
        "priority": "medium",
        "details": "Implement control point identification system that prioritizes girth welds and includes valves/tees/bends from feature_type field. For matching, use joint_number as primary key when available, otherwise fall back to ordered sequence matching with type compatibility and spacing constraints. Use Hungarian algorithm via scipy.optimize.linear_sum_assignment for optimal one-to-one matching with cost matrix based on position differences, type compatibility, and spacing violations. Store both matched and unmatched control points for alignment report generation. This feeds into the piecewise linear alignment step in Task 2.",
        "testStrategy": "Test control point identification with various feature types. Verify joint_number matching when available and sequence-based fallback. Compare Hungarian vs greedy results for control point matching. Test with datasets having different control point densities and distributions.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement control point identification function",
            "description": "Create function to identify control points from pipeline features, prioritizing girth welds and including valves/tees/bends",
            "dependencies": [],
            "details": "Implement identify_control_points function that filters features by type, prioritizing girth welds as primary control points. Include valves, tees, and bends from feature_type field as secondary control points. Extract relevant attributes including joint_number when available, distance, and feature_type. Return structured control point objects with consistent schema for matching.",
            "status": "pending",
            "testStrategy": "Test with datasets containing various feature types. Verify girth weld prioritization and secondary feature inclusion. Test handling of missing joint_number fields.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement joint number matching logic",
            "description": "Create primary matching system using joint_number when available for direct control point correspondence",
            "dependencies": [
              1
            ],
            "details": "Implement joint_number_match function that creates direct matches between control points sharing the same joint_number. Handle cases where joint_number is missing or inconsistent between runs. Return matched pairs and remaining unmatched control points for secondary matching algorithms.",
            "status": "pending",
            "testStrategy": "Test with datasets having complete, partial, and missing joint_number data. Verify direct matching accuracy and proper handling of unmatched points.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement sequence-based matching with type and spacing constraints",
            "description": "Create fallback matching system using ordered sequence with feature type compatibility and spacing constraint validation",
            "dependencies": [
              1,
              2
            ],
            "details": "Implement sequence_match function for control points without joint_number matches. Use ordered sequence position along pipeline with type compatibility checks (girth weld to girth weld preferred). Add spacing constraint validation to prevent unrealistic matches. Calculate cost matrix incorporating position differences, type mismatches, and spacing violations.",
            "status": "pending",
            "testStrategy": "Test sequence matching with various control point distributions. Verify type compatibility enforcement and spacing constraint validation. Test edge cases with sparse control points.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Integrate Hungarian algorithm for optimal control point assignment",
            "description": "Apply Hungarian algorithm using scipy.optimize.linear_sum_assignment for optimal control point matching with comprehensive cost matrix",
            "dependencies": [
              3
            ],
            "details": "Create hungarian_control_point_match function using scipy.optimize.linear_sum_assignment. Build cost matrix incorporating position differences, type compatibility penalties, and spacing constraint violations. Handle unequal numbers of control points between runs. Return optimal matched pairs with assignment costs.",
            "status": "pending",
            "testStrategy": "Compare Hungarian vs greedy control point matching results. Test with various control point densities and distributions. Verify optimal assignment properties for control point scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement control point storage and reporting system",
            "description": "Create system to store matched and unmatched control points for alignment report generation and piecewise linear alignment integration",
            "dependencies": [
              4
            ],
            "details": "Implement ControlPointResults class to store matched pairs, unmatched control points from each run, and matching statistics. Include methods to export control point data for alignment report generation and integration with piecewise linear alignment in Task 2. Add validation and summary statistics for control point matching quality.",
            "status": "pending",
            "testStrategy": "Verify complete storage of matched and unmatched control points. Test integration with alignment report generation. Validate data format compatibility with piecewise linear alignment requirements.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": 4,
        "title": "Add Advanced Matching Features",
        "description": "Implement piecewise linear distance alignment with many-to-one matching for coalescence detection and handle pipeline wrapping edge cases",
        "status": "pending",
        "dependencies": [
          "3"
        ],
        "priority": "medium",
        "details": "Implement piecewise linear distance alignment using matched control points. For each consecutive pair of matched control points, compute scale = (A1-A0)/(B1-B0) and shift = A0 - scale*B0. Apply corrected_distance_B = scale*distance_B + shift for all Run B values in that segment. Store per-segment scale/shift and residuals at control points. Implement fallback to global affine (single shift+scale) if fewer than 2 control points match. Extend matching to support many-to-one scenarios where multiple run1 anomalies match to single run2 anomaly (coalescence). Implement pipeline wrapping logic for features near start/end (distance 0 and max_distance). Add configurable coalescence detection threshold. Use networkx or custom graph algorithms for complex matching scenarios.",
        "testStrategy": "Test piecewise linear alignment with various control point configurations. Verify fallback to global affine when insufficient control points. Test coalescence scenarios with synthetic data. Verify wrapping logic with features at pipeline boundaries. Test edge cases with multiple coalescence candidates.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Piecewise Linear Distance Alignment",
            "description": "Create piecewise linear alignment system using matched control points for distance correction",
            "dependencies": [],
            "details": "Implement piecewise linear distance alignment where for each consecutive pair of matched control points, compute scale = (A1-A0)/(B1-B0) and shift = A0 - scale*B0. Apply corrected_distance_B = scale*distance_B + shift for all Run B values in that segment. Store per-segment scale/shift parameters and residuals at control points for quality assessment.",
            "status": "pending",
            "testStrategy": "Test with various control point configurations and verify correct piecewise alignment calculations",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Global Affine Fallback",
            "description": "Add fallback to global affine transformation when insufficient control points are available",
            "dependencies": [
              1
            ],
            "details": "Implement fallback mechanism that uses global affine transformation (single shift+scale) when fewer than 2 control points match successfully. Calculate global scale and shift parameters using all available matched points. Ensure smooth transition between piecewise and global alignment modes.",
            "status": "pending",
            "testStrategy": "Test fallback behavior with 0, 1, and 2+ control points and verify correct alignment method selection",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Pipeline Wrapping Distance Calculation",
            "description": "Create function to calculate wrapped distances for features near pipeline start/end boundaries",
            "dependencies": [
              2
            ],
            "details": "Implement wrapped distance calculation logic for features at pipeline boundaries (distance 0 and max_distance). Use formula: if abs(distance1 - distance2) > (pipeline_length * 0.9): wrapped_distance = min(distance1, distance2) + (pipeline_length - max(distance1, distance2)). Integrate with piecewise linear alignment system.",
            "status": "pending",
            "testStrategy": "Test with features at boundaries (0, max_distance) and verify wrapped distance calculations work with piecewise alignment",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add Configurable Coalescence Detection Threshold",
            "description": "Implement configurable threshold parameter for detecting when multiple anomalies coalesce into one",
            "dependencies": [
              3
            ],
            "details": "Add coalescence_threshold parameter to configuration. Implement logic to detect when multiple run1 anomalies should be matched to a single run2 anomaly based on proximity and similarity criteria after piecewise alignment correction. Use threshold to determine coalescence candidates within specified distance/depth ranges.",
            "status": "pending",
            "testStrategy": "Test coalescence detection with various threshold values after piecewise alignment and verify correct grouping",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Graph-Based Many-to-One Matching Algorithm",
            "description": "Create graph algorithm using networkx to handle complex many-to-one matching scenarios with aligned distances",
            "dependencies": [
              4
            ],
            "details": "Use networkx to create bipartite graph with run1 anomalies as one set and run2 anomalies as another. Implement weighted edges based on piecewise-aligned distance, depth similarity, and feature type matching. Use maximum weight matching or custom algorithm to handle many-to-one scenarios where multiple run1 features match to single run2 feature.",
            "status": "pending",
            "testStrategy": "Test with synthetic data containing known coalescence scenarios using piecewise-aligned distances and verify correct many-to-one matching results",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": 5,
        "title": "Enhance Growth Rate Calculations",
        "description": "Implement segment-wise anomaly matching with Hungarian algorithm for optimal one-to-one assignment between aligned segments using configurable thresholds and cost functions",
        "status": "pending",
        "dependencies": [
          "4"
        ],
        "priority": "high",
        "details": "Implement segment-wise anomaly matching using Hungarian algorithm between control points. Apply candidate gating with configurable tolerances: |delta_distance| <= DIST_TOL (default 10ft), clock_distance <= CLOCK_TOL (default 15deg), compatible feature type, and orientation match. Build sparse candidate list to avoid dense N×M computation. Use cost function: w_dist*norm(delta_distance) + w_clock*norm(clock_delta) + w_depth*norm(delta_depth) + w_size*norm(delta_length,delta_width) + type_penalty. Solve one-to-one matching per segment using scipy.optimize.linear_sum_assignment. Apply thresholding: best_cost > COST_THRESH => UNCERTAIN status. Handle unmatched cases: Run A unmatched => MISSING, Run B unmatched => NEW.",
        "testStrategy": "Test Hungarian algorithm matching with various segment configurations. Validate candidate gating with different tolerance settings. Test cost function weighting and thresholding logic. Verify handling of unmatched anomalies and status assignment.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Candidate Gating System",
            "description": "Implement configurable candidate filtering based on distance, clock position, feature type, and orientation constraints",
            "dependencies": [],
            "details": "Implement candidate gating with configurable tolerances: |delta_distance| <= DIST_TOL (default 10ft), clock_distance <= CLOCK_TOL (default 15deg). Add feature type compatibility checking and orientation matching. Build sparse candidate list instead of dense N×M matrix to improve performance. Include validation for tolerance parameters and edge case handling.",
            "status": "pending",
            "testStrategy": "Test gating with various tolerance settings. Verify sparse candidate list generation. Test feature type compatibility and orientation matching logic with different anomaly types.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Multi-Component Cost Function",
            "description": "Develop weighted cost function incorporating distance, clock position, depth, size, and type penalties",
            "dependencies": [
              1
            ],
            "details": "Implement cost function: w_dist*norm(delta_distance) + w_clock*norm(clock_delta) + w_depth*norm(delta_depth) + w_size*norm(delta_length,delta_width) + type_penalty. Add normalization functions for each component. Implement configurable weight parameters with sensible defaults. Include type penalty matrix for incompatible feature types.",
            "status": "pending",
            "testStrategy": "Test cost function components individually and in combination. Verify normalization accuracy across different value ranges. Test weight parameter sensitivity and type penalty assignments.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Integrate Hungarian Algorithm for Optimal Assignment",
            "description": "Implement segment-wise Hungarian algorithm matching using scipy.optimize.linear_sum_assignment",
            "dependencies": [
              2
            ],
            "details": "Integrate scipy.optimize.linear_sum_assignment for one-to-one optimal matching per segment. Build cost matrix from candidate pairs and cost function. Handle segments with no valid candidates. Implement per-segment processing between control points with proper indexing and result aggregation.",
            "status": "pending",
            "testStrategy": "Test Hungarian algorithm with various cost matrices. Verify one-to-one assignment constraints. Test segment-wise processing with different control point configurations and candidate distributions.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Cost Thresholding and Status Assignment",
            "description": "Add configurable cost thresholding and automatic status assignment for matched and unmatched anomalies",
            "dependencies": [
              3
            ],
            "details": "Implement cost thresholding: best_cost > COST_THRESH => UNCERTAIN status. Handle unmatched anomalies: Run A unmatched => MISSING, Run B unmatched => NEW. Add configurable COST_THRESH parameter with validation. Implement status assignment logic and result aggregation across all segments.",
            "status": "pending",
            "testStrategy": "Test cost thresholding with various threshold values. Verify correct status assignment for matched, uncertain, missing, and new anomalies. Test edge cases with all matched or all unmatched scenarios.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Configuration Management and Performance Optimization",
            "description": "Implement configuration system for matching parameters and optimize performance for large datasets",
            "dependencies": [
              4
            ],
            "details": "Add configuration management for DIST_TOL, CLOCK_TOL, COST_THRESH, and weight parameters. Implement performance optimizations for sparse matrix operations and large segment processing. Add progress tracking and memory usage monitoring. Include parameter validation and sensible defaults.",
            "status": "pending",
            "testStrategy": "Test configuration loading and parameter validation. Benchmark performance with large datasets. Verify memory usage optimization and progress tracking accuracy. Test with various parameter combinations.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": 6,
        "title": "Implement Advanced Reporting and Visualization",
        "description": "Implement growth rate calculations and generate HTML/PDF reports with charts, summary statistics, and dig-list prioritization",
        "status": "pending",
        "dependencies": [
          "5"
        ],
        "priority": "medium",
        "details": "First implement growth rate calculations in src/growth.py: compute depth_growth_pct_per_year = (depth_B - depth_A) / years_between, length_growth_in_per_year, and width_growth_in_per_year. Derive years_between from run_id metadata or accept via CLI --years_between param. Handle missing fields gracefully (NaN when field absent) and flag anomalies with negative growth (possible measurement error). Then use matplotlib/plotly for visualization and reportlab/weasyprint for PDF generation. Create summary statistics: mean/median/max growth rates by feature_type and pipeline segment. Generate depth growth distribution histograms, worst-N anomalies charts. Implement dig-list ranking by severity score combining growth rate, current depth, and remaining life. Export to HTML with interactive charts using plotly. Example: fig = px.histogram(df, x='depth_growth_pct_per_year', color='feature_type'); report_html = generate_html_template(summary_stats, figures, dig_list)",
        "testStrategy": "Test growth rate calculations with known time intervals and depth changes. Verify handling of missing fields and negative growth flagging. Verify report generation with sample data. Test PDF/HTML output formatting. Validate dig-list ranking algorithm and chart accuracy.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Growth Rate Calculation Module",
            "description": "Create src/growth.py with functions to calculate depth, length, and width growth rates per year",
            "dependencies": [],
            "details": "Implement growth rate calculations: depth_growth_pct_per_year = (depth_B - depth_A) / years_between, length_growth_in_per_year, and width_growth_in_per_year. Derive years_between from run_id metadata or accept via CLI --years_between parameter. Handle missing fields gracefully by returning NaN when fields are absent. Flag anomalies with negative growth rates as possible measurement errors. Include validation for reasonable time intervals and growth rate bounds.",
            "status": "pending",
            "testStrategy": "Unit tests with known depth changes and time intervals. Test missing field handling and NaN generation. Verify negative growth flagging and CLI parameter integration.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement Summary Statistics Calculation Module",
            "description": "Create functions to calculate mean, median, and maximum growth rates grouped by feature_type and pipeline segment",
            "dependencies": [
              1
            ],
            "details": "Develop statistical analysis functions using pandas groupby operations to compute summary statistics for the calculated growth rates. Calculate mean/median/max growth rates for each feature_type (e.g., metal loss, dent, crack) and pipeline segment using the new growth rate fields. Include standard deviation and count metrics. Handle missing values and edge cases gracefully.",
            "status": "pending",
            "testStrategy": "Unit tests with sample datasets containing various feature types and segments. Verify statistical calculations against manual computations using the new growth rate fields.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Create Interactive Charts with Plotly",
            "description": "Develop visualization functions for depth growth distribution histograms and worst-N anomalies charts using plotly",
            "dependencies": [
              2
            ],
            "details": "Implement plotly-based interactive charts including histograms for depth_growth_pct_per_year distribution colored by feature_type, scatter plots for worst anomalies, and bar charts for summary statistics. Create reusable chart generation functions with customizable parameters for colors, titles, and data filtering. Include charts for length and width growth rates as well.",
            "status": "pending",
            "testStrategy": "Visual testing with sample data to verify chart accuracy and interactivity. Test different data sizes and edge cases with empty datasets. Verify proper handling of NaN values in visualizations.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Implement Dig-List Ranking Algorithm",
            "description": "Create severity scoring system combining growth rate, current depth, and remaining life for dig-list prioritization",
            "dependencies": [
              2
            ],
            "details": "Develop weighted scoring algorithm that combines normalized depth_growth_pct_per_year, current depth percentage, and calculated remaining life. Implement configurable weights for each factor. Sort anomalies by severity score and generate ranked dig-list with priority levels (critical, high, medium, low). Handle cases where growth rates are negative (flagged measurement errors).",
            "status": "pending",
            "testStrategy": "Test ranking algorithm with known scenarios including negative growth rates. Verify score calculations and priority assignments. Test with different weight configurations.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Build HTML Report Generation System",
            "description": "Create HTML template system with embedded interactive plotly charts and formatted summary tables",
            "dependencies": [
              3,
              4
            ],
            "details": "Develop HTML template engine using Jinja2 or similar to generate comprehensive reports. Embed plotly charts as interactive HTML elements showing growth rate distributions and anomaly rankings. Create formatted tables for summary statistics and dig-list. Include CSS styling for professional appearance and responsive design. Display flagged negative growth anomalies prominently.",
            "status": "pending",
            "testStrategy": "Test HTML generation with various data sets including negative growth cases. Verify chart embedding and interactivity in browsers. Test responsive design on different screen sizes.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement PDF Export Functionality",
            "description": "Add PDF generation capability using reportlab or weasyprint with static charts and formatted tables",
            "dependencies": [
              5
            ],
            "details": "Implement PDF export using reportlab or weasyprint libraries. Convert interactive plotly charts to static images for PDF inclusion. Create professional PDF layout with headers, footers, page numbers, and proper formatting. Handle multi-page reports and table pagination. Include section for flagged negative growth anomalies.",
            "status": "pending",
            "testStrategy": "Test PDF generation with large datasets including negative growth cases. Verify chart quality and table formatting. Test pagination and layout consistency across different report sizes.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": 7,
        "title": "Add Support for Non-Linear Growth Models",
        "description": "Implement non-linear growth modeling when 3+ historical runs are available and create comprehensive output artifacts and reporting system",
        "status": "pending",
        "dependencies": [
          "5"
        ],
        "priority": "low",
        "details": "Extend growth calculations to support exponential, polynomial, and power-law models when multiple historical runs (3+) are available. Use scipy.optimize.curve_fit for model fitting. Implement model selection based on R-squared and AIC criteria. Support growth models: linear, exponential (a*exp(b*t)), polynomial (a + b*t + c*t^2), power-law (a*t^b). Create comprehensive reporting system in src/reporting.py that generates: matched_results.csv (one row per matched pair with RunA fields, RunB fields, corrected_distance_B, deltas, growth rates, confidence, status MATCHED/UNCERTAIN), new_anomalies.csv (RunB-only with canonical fields), missing_anomalies.csv (RunA-only with canonical fields), alignment_report.json (matched control points, per-segment transforms with scale/shift, residuals, parameters used including tolerances and weights, mapping decisions summary). Print summary statistics including anomaly counts per run, matched, uncertain, new, missing, and top 10 fastest-growing anomalies.",
        "testStrategy": "Test model fitting with synthetic multi-run data. Verify model selection criteria. Test extrapolation accuracy and confidence intervals. Test CSV and JSON output generation with various data scenarios. Verify summary statistics calculations and fastest-growing anomaly identification.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Mathematical Model Functions",
            "description": "Create functions for exponential, polynomial, and power-law growth models",
            "dependencies": [],
            "details": "Implement mathematical functions for each growth model type: exponential (a*exp(b*t)), polynomial (a + b*t + c*t^2), and power-law (a*t^b). Create a unified interface for all model types with proper parameter initialization and bounds. Include error handling for invalid parameters and numerical instabilities.",
            "status": "pending",
            "testStrategy": "Unit test each model function with known parameter sets and verify mathematical correctness",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Integrate scipy.optimize.curve_fit for Model Fitting",
            "description": "Implement curve fitting functionality using scipy for parameter estimation",
            "dependencies": [
              1
            ],
            "details": "Use scipy.optimize.curve_fit to fit each model type to historical depth data. Implement proper initial parameter guessing, bounds setting, and convergence handling. Add robust error handling for fitting failures and parameter estimation uncertainties using covariance matrix.",
            "status": "pending",
            "testStrategy": "Test fitting with synthetic data of known parameters and verify parameter recovery accuracy",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Model Selection Criteria",
            "description": "Create R-squared and AIC calculation functions for model comparison",
            "dependencies": [
              2
            ],
            "details": "Implement R-squared calculation for goodness-of-fit assessment and Akaike Information Criterion (AIC) for model selection. Create a model comparison function that ranks models based on both criteria. Include handling for edge cases where models fail to converge or produce invalid results.",
            "status": "pending",
            "testStrategy": "Validate R-squared and AIC calculations against known statistical examples and verify model ranking logic",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create Multi-Run Data Validation and Processing",
            "description": "Implement validation logic to ensure 3+ historical runs are available",
            "dependencies": [],
            "details": "Create validation functions to check for minimum 3 historical runs requirement. Implement data preprocessing to extract time points and depth values from multiple runs. Add data quality checks for sufficient data points per run and temporal consistency across runs.",
            "status": "pending",
            "testStrategy": "Test validation with various data scenarios including insufficient runs and malformed data",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Integrate Non-Linear Models into Growth Calculation Pipeline",
            "description": "Extend existing growth calculations to support non-linear model selection and prediction",
            "dependencies": [
              3,
              4
            ],
            "details": "Modify the main growth calculation pipeline to automatically select the best-fitting model when 3+ runs are available. Implement fallback to linear model for insufficient data. Add confidence interval calculation for predictions and integrate with existing anomaly tracking workflow.",
            "status": "pending",
            "testStrategy": "Test end-to-end pipeline with multi-run synthetic data and verify model selection and prediction accuracy",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Create Comprehensive Reporting System (src/reporting.py)",
            "description": "Implement output artifacts generation including CSV files and JSON alignment report",
            "dependencies": [
              5
            ],
            "details": "Create src/reporting.py module to generate comprehensive output artifacts: matched_results.csv with one row per matched pair containing RunA fields, RunB fields, corrected_distance_B, deltas, growth rates, confidence, and status (MATCHED/UNCERTAIN); new_anomalies.csv for RunB-only anomalies with canonical fields; missing_anomalies.csv for RunA-only anomalies with canonical fields; alignment_report.json containing matched control points, per-segment transforms with scale/shift values, residuals, parameters used including tolerances and weights, and mapping decisions summary.",
            "status": "pending",
            "testStrategy": "Test CSV and JSON file generation with various matching scenarios and verify correct field mapping and data integrity",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Implement Summary Statistics and Console Output",
            "description": "Create summary statistics calculation and console reporting functionality",
            "dependencies": [
              6
            ],
            "details": "Implement summary statistics calculation including anomaly counts per run, matched count, uncertain count, new anomalies count, missing anomalies count, and identification of top 10 fastest-growing anomalies based on growth rates. Create formatted console output that displays these statistics in a clear, readable format for immediate feedback to users.",
            "status": "pending",
            "testStrategy": "Test summary statistics calculations with known data sets and verify correct identification of fastest-growing anomalies and accurate count reporting",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": 8,
        "title": "Develop Comprehensive Test Suite",
        "description": "Create unit and integration tests covering all core functionality and edge cases, including comprehensive testing of the CLI entry point run_pipeline.py",
        "status": "pending",
        "dependencies": [
          "7"
        ],
        "priority": "high",
        "details": "Write comprehensive test suite using pytest framework. Unit tests for: clock_to_degrees, angular_difference, normalise_orientation, alignment calculations, matching algorithms. Integration tests with known sample data and expected results. CLI testing for run_pipeline.py with various argument combinations and file formats. Edge case tests: empty runs, single anomaly, all unmatched, identical runs, pipeline wrapping, invalid CLI arguments, missing files. Use pytest fixtures for test data setup. Achieve >90% code coverage. Example: @pytest.fixture def sample_data(): return pd.DataFrame({'feature_id': [1,2], 'distance': [100,200], ...}). Test CLI usage: python run_pipeline.py --file ILIDataV2.xlsx --run_a 2015 --run_b 2022 --outdir outputs/ with optional flags.",
        "testStrategy": "Run pytest with coverage reporting. Test all functions with boundary conditions. Verify integration test results match expected outputs. Test CLI entry point with subprocess calls and mock file systems. Validate output directory creation and file generation.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Unit Tests for Core Mathematical Functions",
            "description": "Implement comprehensive unit tests for all mathematical utility functions including clock_to_degrees, angular_difference, and normalise_orientation",
            "dependencies": [],
            "details": "Write pytest unit tests for clock_to_degrees (test 12 o'clock = 0°, 3 o'clock = 90°, etc.), angular_difference (test wraparound cases like 350° to 10°), and normalise_orientation (test negative angles, angles > 360°). Include boundary value testing and invalid input handling. Use parametrized tests for comprehensive coverage.",
            "status": "pending",
            "testStrategy": "Use pytest.mark.parametrize for multiple test cases. Test boundary conditions and invalid inputs. Verify mathematical accuracy with known values.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Develop Unit Tests for Alignment Calculations",
            "description": "Create unit tests for distance alignment and multi-point interpolation functions with various pipeline configurations",
            "dependencies": [
              1
            ],
            "details": "Test alignment calculation functions including constant offset alignment and piecewise linear interpolation. Create test cases for single reference point, multiple reference points, and edge cases like pipeline wrapping. Mock scipy.interpolate.interp1d behavior and test stretch factor calculations between segments.",
            "status": "pending",
            "testStrategy": "Test with synthetic data having known stretch factors. Verify interpolation accuracy and fallback to constant offset when needed.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Implement Unit Tests for Matching Algorithms",
            "description": "Build comprehensive unit tests for anomaly matching algorithms including one-to-one, many-to-one, and coalescence detection",
            "dependencies": [
              2
            ],
            "details": "Test matching algorithms with various scenarios: perfect matches, partial matches, unmatched anomalies, and coalescence cases. Include tests for distance-based matching, orientation matching, and combined scoring. Test configurable thresholds and matching criteria. Create edge cases with empty datasets and single anomalies.",
            "status": "pending",
            "testStrategy": "Use pytest fixtures for test data setup. Test all matching scenarios with known expected results. Verify threshold behavior and scoring accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create Integration Tests with Sample Data",
            "description": "Develop integration tests using realistic pipeline inspection data to verify end-to-end functionality",
            "dependencies": [
              3
            ],
            "details": "Create pytest fixtures with realistic sample data including multiple pipeline runs with known anomalies. Test complete workflow from data loading through matching to output generation. Include tests for different input formats (CSV, Excel) and verify output structure matches expected schema. Test with actual pipeline data scenarios.",
            "status": "pending",
            "testStrategy": "Use pytest fixtures for sample data setup. Compare integration test results with manually verified expected outputs. Test complete data pipeline flow.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement Edge Case and Coverage Testing",
            "description": "Create comprehensive edge case tests and achieve >90% code coverage with pytest-cov reporting",
            "dependencies": [
              4
            ],
            "details": "Implement edge case tests for: empty runs, single anomaly datasets, all unmatched scenarios, identical runs, pipeline wrapping cases, and invalid input data. Set up pytest-cov for coverage reporting and ensure >90% coverage target. Add tests for error handling and exception cases. Create performance tests for large datasets.",
            "status": "pending",
            "testStrategy": "Run pytest with coverage reporting using pytest-cov. Test all edge cases and boundary conditions. Verify error handling and exception paths are covered.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Develop CLI Entry Point Tests for run_pipeline.py",
            "description": "Create comprehensive tests for the CLI entry point including argument parsing, file handling, and end-to-end pipeline execution",
            "dependencies": [
              5
            ],
            "details": "Test run_pipeline.py CLI with various argument combinations: required args (--file, --run_a, --run_b, --outdir) and optional flags (--dist_tol, --clock_tol, --cost_thresh, --years_between, --enable_confidence, --enable_multirun). Test with different file formats (Excel, CSV), invalid file paths, missing arguments, and invalid argument values. Use subprocess.run() to test actual CLI execution and verify output directory creation and file generation. Mock file system operations where appropriate.",
            "status": "pending",
            "testStrategy": "Use subprocess calls to test CLI execution. Mock file operations and verify argument parsing. Test error handling for invalid inputs and missing files. Validate output file creation and summary statistics.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Test Pipeline Orchestration and Output Validation",
            "description": "Verify the complete pipeline orchestration flow and validate all output files and summary statistics",
            "dependencies": [
              6
            ],
            "details": "Test the full pipeline orchestration: load data -> preprocess -> identify control points -> align -> match -> compute growth -> write outputs. Validate output file formats, content structure, and summary statistics accuracy. Test with various pipeline configurations and data scenarios. Verify that all intermediate steps are properly connected and data flows correctly through the pipeline. Test output directory structure and file naming conventions.",
            "status": "pending",
            "testStrategy": "Run end-to-end tests with known input data and verify output accuracy. Test pipeline with different configurations and validate intermediate results. Check output file formats and summary statistics calculations.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": 9,
        "title": "Enhance CLI and Add Configuration Management",
        "description": "Write comprehensive tests for the anomaly matching pipeline including unit tests for core algorithms and integration tests for the full pipeline",
        "status": "pending",
        "dependencies": [
          "8"
        ],
        "priority": "medium",
        "details": "Implement comprehensive test suite covering all core functionality. Unit tests for: clock parsing (clock_to_degrees, angular_difference), column mapping auto-detection, control point identification, piecewise linear alignment math, cost function computation, Hungarian matching algorithm. Integration test to run full pipeline on sample data with known expected output. Edge case testing for: empty runs, single anomaly, all unmatched scenarios, identical runs, runs with no control points (fallback to global affine). Use pytest framework with fixtures for test data and mock objects for file I/O operations.",
        "testStrategy": "Unit tests with pytest for individual functions and algorithms. Integration tests with sample datasets and known outputs. Edge case testing with boundary conditions and error scenarios. Mock file operations for isolated testing.",
        "subtasks": [
          {
            "id": 1,
            "title": "Implement Unit Tests for Clock and Angular Functions",
            "description": "Create comprehensive unit tests for clock position parsing and angular difference calculations",
            "dependencies": [],
            "details": "Write unit tests for clock_to_degrees function covering all valid clock positions (1-12) and edge cases. Test angular_difference function with various angle pairs including wraparound cases (e.g., 350° to 10°). Verify correct handling of invalid inputs and boundary conditions. Use pytest parametrize for comprehensive test coverage with multiple input combinations.",
            "status": "pending",
            "testStrategy": "Parametrized tests with all clock positions and angle combinations. Test invalid inputs and boundary conditions.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Create Tests for Column Mapping Auto-Detection",
            "description": "Implement unit tests for automatic column mapping detection functionality",
            "dependencies": [],
            "details": "Test column mapping auto-detection with various CSV/Excel file formats and column naming conventions. Verify correct identification of required fields (distance, clock, depth) and optional fields (feature_type, joint_number). Test handling of missing columns, duplicate column names, and ambiguous mappings. Include tests for case-insensitive matching and common column name variations.",
            "status": "pending",
            "testStrategy": "Test with various file formats and column naming patterns. Verify error handling for missing or ambiguous columns.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Develop Control Point Identification Tests",
            "description": "Create unit tests for control point identification and matching algorithms",
            "dependencies": [],
            "details": "Test control point identification logic for girth welds, valves, tees, and bends. Verify joint_number matching when available and sequence-based fallback matching. Test Hungarian algorithm implementation for optimal control point assignment. Include edge cases with no control points, single control point, and mismatched control point counts between runs.",
            "status": "pending",
            "testStrategy": "Test control point detection with various feature types. Verify matching algorithms with different control point distributions.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Test Piecewise Linear Alignment Mathematics",
            "description": "Implement comprehensive tests for piecewise linear interpolation and alignment calculations",
            "dependencies": [
              3
            ],
            "details": "Test piecewise linear interpolation using scipy.interpolate.interp1d with known control points and expected stretch factors. Verify alignment accuracy between reference points and handling of extrapolation beyond control points. Test fallback to global affine transformation when no control points are available. Include tests for segment-wise stretch factor calculations and interpolation edge cases.",
            "status": "pending",
            "testStrategy": "Test interpolation accuracy with synthetic data having known stretch factors. Verify fallback mechanisms and edge case handling.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Create Cost Function and Hungarian Algorithm Tests",
            "description": "Develop unit tests for cost function computation and Hungarian matching algorithm implementation",
            "dependencies": [
              4
            ],
            "details": "Test cost function calculation with various weight combinations for distance, clock, depth, size, and type penalties. Verify Hungarian algorithm implementation using scipy.optimize.linear_sum_assignment with different cost matrices. Test candidate gating logic with configurable tolerances and sparse matrix generation. Include tests for unmatched anomaly handling and optimal assignment verification.",
            "status": "pending",
            "testStrategy": "Test cost function with various parameter combinations. Verify Hungarian algorithm optimality and candidate gating effectiveness.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement Full Pipeline Integration Tests",
            "description": "Create integration tests for the complete anomaly matching pipeline with sample datasets",
            "dependencies": [
              1,
              2,
              3,
              4,
              5
            ],
            "details": "Develop integration test that runs the full pipeline from data loading through final output generation using sample datasets with known expected results. Test complete workflow including data validation, control point identification, alignment, and anomaly matching. Verify output format consistency and accuracy against expected results. Include performance benchmarking for large datasets.",
            "status": "pending",
            "testStrategy": "End-to-end pipeline testing with sample data and known outputs. Performance testing with large datasets.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Develop Edge Case and Error Handling Tests",
            "description": "Create comprehensive tests for edge cases and error conditions throughout the pipeline",
            "dependencies": [
              6
            ],
            "details": "Test edge cases including empty runs, single anomaly scenarios, all unmatched anomalies, identical runs, and runs with no control points. Verify proper fallback to global affine transformation when piecewise alignment fails. Test error handling for malformed input data, file access issues, and invalid configuration parameters. Include tests for graceful degradation and meaningful error messages.",
            "status": "pending",
            "testStrategy": "Comprehensive edge case testing with boundary conditions. Error handling verification with invalid inputs and system failures.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": 10,
        "title": "Add Pipeline Operator Standard Export Formats",
        "description": "Create comprehensive README.md documentation with usage instructions, parameter explanations, and troubleshooting guide for the pipeline anomaly matching system",
        "status": "pending",
        "dependencies": [
          "6"
        ],
        "priority": "medium",
        "details": "Write comprehensive README.md documentation covering: one-command run instructions (python run_pipeline.py --file ILIDataV2.xlsx --run_a 2015 --run_b 2022 --outdir outputs/), assumptions and parameter defaults (dist_tol=10ft, clock_tol=15deg, etc.), explanation of alignment method (piecewise linear), matching method (segment-wise Hungarian), confidence scoring approach, output file definitions (matched_results.csv, new_anomalies.csv, missing_anomalies.csv, alignment_report.json), troubleshooting common data issues (missing columns, wrong sheet names, Excel format quirks). Include examples of typical usage scenarios and expected output formats.",
        "testStrategy": "Verify README instructions work with sample data. Test documented parameter defaults and troubleshooting scenarios. Validate output file descriptions match actual generated files.",
        "subtasks": [
          {
            "id": 1,
            "title": "Create Usage Instructions and Command Reference",
            "description": "Document the main command-line interface with all parameters and provide clear usage examples",
            "dependencies": [],
            "details": "Write comprehensive usage section covering the main command: python run_pipeline.py --file ILIDataV2.xlsx --run_a 2015 --run_b 2022 --outdir outputs/. Document all command-line parameters including required and optional arguments. Provide multiple usage examples for different scenarios (basic matching, custom tolerances, different file formats). Include parameter validation and error handling examples.",
            "status": "pending",
            "testStrategy": "Test all documented command examples and verify parameter descriptions match actual implementation",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Document Algorithm Methods and Parameters",
            "description": "Explain the piecewise linear alignment method, Hungarian matching algorithm, and all parameter defaults with their meanings",
            "dependencies": [],
            "details": "Document the piecewise linear alignment method explaining how control points are used for segmented distance correction. Explain the segment-wise Hungarian algorithm for optimal anomaly matching. Document all parameter defaults: dist_tol=10ft, clock_tol=15deg, and other tolerance settings. Explain confidence scoring approach and how matching quality is assessed. Include algorithm flow diagrams and mathematical explanations where helpful.",
            "status": "pending",
            "testStrategy": "Verify algorithm descriptions match implementation and parameter defaults are accurate",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Define Output File Formats and Contents",
            "description": "Document all output files with detailed field descriptions and example content",
            "dependencies": [],
            "details": "Create comprehensive documentation for all output files: matched_results.csv (matched anomaly pairs with confidence scores), new_anomalies.csv (Run B anomalies without matches), missing_anomalies.csv (Run A anomalies without matches), alignment_report.json (control point matches and alignment statistics). Include field-by-field descriptions, data types, and example rows for each output file. Document the relationship between files and how they should be interpreted together.",
            "status": "pending",
            "testStrategy": "Generate sample outputs and verify documentation matches actual file contents and field names",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Create Troubleshooting Guide for Common Issues",
            "description": "Document solutions for common data problems and Excel format issues users may encounter",
            "dependencies": [],
            "details": "Create comprehensive troubleshooting section covering: missing required columns and how to handle them, wrong Excel sheet names and sheet selection, Excel format quirks (merged cells, hidden rows, formatting issues), data validation failures and how to fix them, common parameter tuning scenarios, performance issues with large datasets. Include specific error messages users might see and their solutions. Provide data preparation guidelines and best practices.",
            "status": "pending",
            "testStrategy": "Test documented solutions with problematic sample files and verify error scenarios are handled as described",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Add Installation and Dependencies Section",
            "description": "Document installation requirements, dependencies, and environment setup instructions",
            "dependencies": [],
            "details": "Create installation section covering Python version requirements, required packages (pandas, scipy, numpy, openpyxl, etc.), installation via pip or conda, virtual environment setup recommendations. Include system requirements and known compatibility issues. Document how to verify installation success and run basic functionality tests. Provide troubleshooting for common installation problems.",
            "status": "pending",
            "testStrategy": "Test installation instructions on clean environments and verify all dependencies are correctly documented",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": "11",
        "title": "Create Comprehensive Documentation",
        "description": "Add docstrings, README, and usage examples with complete API documentation including new probabilistic scoring and confidence scoring features",
        "status": "pending",
        "dependencies": [
          "10"
        ],
        "priority": "medium",
        "details": "Add Google-style docstrings to all public functions and classes. Create comprehensive README.md with installation, usage examples, input format specifications, and output field descriptions. Document configuration file schema and CLI options including new --enable_confidence flag. Include example workflow with sample data walkthrough. Use Sphinx for API documentation generation. Document new probabilistic scoring using Gaussian error model: score = exp(-[(delta_dist/sigma_d)^2 + (delta_clock/sigma_c)^2 + (delta_depth/sigma_depth)^2]) * type_match * orientation_match, where sigma_d=5ft, sigma_c=15deg, sigma_depth=10pct are tunable parameters. Also document sigmoid-based confidence using margin: confidence = sigmoid(alpha*(-best_cost) + beta*(margin) - gamma*(candidate_count)) where margin = second_best_cost - best_cost. Document both match_probability (Gaussian) and match_confidence (margin-based) outputs with High/Medium/Low confidence categorization. Document explainability fields: dist_delta_ft, clock_delta_deg, depth_delta_pct, len_delta, width_delta, best_cost, margin, candidate_count, segment_id. Example docstring format: '''Calculate corrosion growth rate. Args: depth1 (float): Initial depth percentage. depth2 (float): Final depth percentage. time_diff (float): Time difference in years. Returns: float: Growth rate in %/year'''",
        "testStrategy": "Verify docstring completeness with pydocstyle. Test README examples with fresh environment including probabilistic scoring and confidence scoring examples. Generate and review Sphinx documentation. Test both Gaussian probability and sigmoid confidence documentation with sample outputs.",
        "subtasks": [
          {
            "id": 1,
            "title": "Add Google-style docstrings to all public functions and classes",
            "description": "Implement comprehensive Google-style docstrings for all public functions and classes in the codebase including probabilistic scoring and confidence scoring functions",
            "dependencies": [],
            "details": "Add Google-style docstrings to all public functions and classes following the specified format. Include Args, Returns, Raises sections where applicable. Add specific documentation for probabilistic scoring functions including Gaussian error model parameters (sigma_d, sigma_c, sigma_depth) and confidence scoring functions including sigmoid formula parameters (alpha, beta, gamma) and explainability field calculations. Document both match_probability and match_confidence outputs. Example format: '''Calculate corrosion growth rate. Args: depth1 (float): Initial depth percentage. depth2 (float): Final depth percentage. time_diff (float): Time difference in years. Returns: float: Growth rate in %/year'''",
            "status": "pending",
            "testStrategy": "Use pydocstyle to verify docstring completeness and format compliance including probabilistic scoring and confidence scoring functions"
          },
          {
            "id": 2,
            "title": "Create comprehensive README.md with installation and usage examples",
            "description": "Develop a complete README.md file with installation instructions, usage examples, and input/output specifications including probabilistic scoring and confidence scoring features",
            "dependencies": [
              1
            ],
            "details": "Create comprehensive README.md including installation instructions, basic usage examples, input format specifications (CSV/Excel structure), output field descriptions including new explainability fields (dist_delta_ft, clock_delta_deg, depth_delta_pct, len_delta, width_delta, best_cost, margin, candidate_count, segment_id), and configuration options. Include code examples showing typical workflow with --enable_confidence flag and expected outputs with both match_probability (Gaussian-based) and match_confidence (margin-based) scores with High/Medium/Low categorization.",
            "status": "pending",
            "testStrategy": "Test all README examples in a fresh environment to ensure they work correctly including probabilistic scoring and confidence scoring examples"
          },
          {
            "id": 3,
            "title": "Document configuration file schema and CLI options",
            "description": "Create detailed documentation for configuration file structure and command-line interface options including new probabilistic scoring and confidence scoring parameters",
            "dependencies": [
              1
            ],
            "details": "Document the complete configuration file schema including all available parameters, their types, default values, and descriptions. Document all CLI options with examples including new --enable_confidence flag. Document probabilistic scoring parameters (sigma_d=5ft, sigma_c=15deg, sigma_depth=10pct) and confidence scoring parameters (alpha, beta, gamma) in configuration schema. Create schema validation documentation and parameter reference guide with both Gaussian probability and sigmoid confidence configuration examples.",
            "status": "pending",
            "testStrategy": "Validate configuration examples against actual schema and test CLI option documentation including probabilistic scoring and confidence scoring parameters"
          },
          {
            "id": 4,
            "title": "Create example workflow with sample data walkthrough",
            "description": "Develop a complete example workflow demonstrating the tool's capabilities with sample data including probabilistic scoring and confidence scoring features",
            "dependencies": [
              2,
              3
            ],
            "details": "Create a step-by-step example workflow using sample data that demonstrates input preparation, configuration setup, execution with --enable_confidence flag, and output interpretation including both match_probability (Gaussian) and match_confidence (margin-based) scores with High/Medium/Low categorization and explainability fields. Include sample CSV/Excel files, configuration files with probabilistic and confidence parameters, and expected outputs with explanations of both scoring methods and explainability metrics.",
            "status": "pending",
            "testStrategy": "Execute the complete example workflow to verify all steps work correctly and produce expected results including both probabilistic scoring and confidence scoring outputs"
          },
          {
            "id": 5,
            "title": "Set up Sphinx for API documentation generation",
            "description": "Configure Sphinx to automatically generate API documentation from docstrings including probabilistic scoring and confidence scoring modules",
            "dependencies": [
              1
            ],
            "details": "Set up Sphinx configuration with autodoc extension to generate API documentation from Google-style docstrings. Configure themes, structure, and build process. Create index pages and organize documentation sections including dedicated sections for probabilistic scoring module (Gaussian error model) and confidence scoring module (sigmoid-based). Set up automated documentation building with proper cross-references to both scoring functions and explainability field documentation.",
            "status": "pending",
            "testStrategy": "Generate Sphinx documentation and review for completeness, formatting, and accuracy of API references including both probabilistic scoring and confidence scoring module documentation"
          },
          {
            "id": 6,
            "title": "Document probabilistic scoring algorithm and confidence scoring with explainability fields",
            "description": "Create detailed documentation for both probabilistic scoring and confidence scoring features including mathematical formulations and output field descriptions",
            "dependencies": [
              1
            ],
            "details": "Create comprehensive documentation for probabilistic scoring using Gaussian error model: score = exp(-[(delta_dist/sigma_d)^2 + (delta_clock/sigma_c)^2 + (delta_depth/sigma_depth)^2]) * type_match * orientation_match, where sigma_d=5ft, sigma_c=15deg, sigma_depth=10pct are tunable parameters producing 0-1 probability per candidate pair. Also document sigmoid-based confidence scoring: confidence = sigmoid(alpha*(-best_cost) + beta*(margin) - gamma*(candidate_count)) where margin = second_best_cost - best_cost. Document both match_probability (Gaussian) and match_confidence (margin-based) outputs with High/Medium/Low confidence categorization. Document all explainability fields: dist_delta_ft (distance difference in feet), clock_delta_deg (clock position difference in degrees), depth_delta_pct (depth difference percentage), len_delta (length difference), width_delta (width difference), best_cost (optimal matching cost), margin (cost difference between best and second-best), candidate_count (number of potential matches), segment_id (pipeline segment identifier). Include parameter tuning guidelines and interpretation examples for both scoring methods.",
            "status": "pending",
            "testStrategy": "Verify mathematical accuracy of documented formulas for both Gaussian and sigmoid models and validate explainability field descriptions against actual implementation"
          }
        ]
      },
      {
        "id": 12,
        "title": "Performance Optimization and Memory Management",
        "description": "STRETCH GOAL - Implement multi-run support and optimize performance for large datasets with memory-efficient processing",
        "status": "pending",
        "dependencies": [
          "11"
        ],
        "priority": "low",
        "details": "Profile code using cProfile and memory_profiler to identify bottlenecks. Implement chunked processing for large datasets using pandas.read_csv(chunksize=). Optimize matching algorithms with spatial indexing (scipy.spatial.KDTree) for large anomaly sets. Add progress bars using tqdm. Implement parallel processing for independent calculations using multiprocessing.Pool. STRETCH GOAL: Extend pipeline to handle 3+ runs (2007, 2015, 2022) with multi-run anomaly tracking. Build anomaly tracks across runs by assigning track_id to anomalies representing the same physical feature across multiple inspections. Output tracks_multi_run.csv with track_id and columns for each run's feature_id and measurements, allowing gaps where features were not detected. Enable via --enable_multirun CLI flag with run list. Example: from scipy.spatial import KDTree; tree = KDTree(run1_positions); distances, indices = tree.query(run2_positions, k=5); from multiprocessing import Pool; with Pool() as pool: results = pool.map(calculate_growth_rate, anomaly_pairs)",
        "testStrategy": "Benchmark performance with large synthetic datasets (10k+ anomalies). Monitor memory usage during processing. Verify parallel processing produces identical results to serial processing. Test multi-run tracking with 3+ runs ensuring track continuity and gap handling. Validate tracks_multi_run.csv output format and CLI flag functionality.",
        "subtasks": [
          {
            "id": 1,
            "title": "Profile code performance and identify bottlenecks",
            "description": "Use cProfile and memory_profiler to analyze current code performance and identify computational bottlenecks",
            "dependencies": [],
            "details": "Implement comprehensive profiling using cProfile for CPU performance analysis and memory_profiler for memory usage tracking. Create profiling scripts that run the existing codebase with sample datasets of varying sizes. Generate detailed reports showing function call times, memory consumption patterns, and identify the most resource-intensive operations. Set up automated profiling pipeline that can be run before and after optimizations.",
            "status": "pending",
            "testStrategy": "Run profiling on datasets of different sizes (1k, 5k, 10k anomalies). Compare baseline performance metrics and validate profiling accuracy.",
            "parentId": "undefined"
          },
          {
            "id": 2,
            "title": "Implement chunked processing for large datasets",
            "description": "Add memory-efficient chunked processing using pandas.read_csv with chunksize parameter",
            "dependencies": [
              1
            ],
            "details": "Modify data loading functions to support chunked processing using pandas.read_csv(chunksize=) for large CSV files. Implement chunk aggregation logic that processes data in manageable pieces while maintaining result consistency. Add automatic chunk size determination based on available memory. Create streaming processing pipeline that can handle datasets larger than available RAM without performance degradation.",
            "status": "pending",
            "testStrategy": "Test with large synthetic datasets exceeding available memory. Verify identical results between chunked and non-chunked processing. Monitor memory usage during processing.",
            "parentId": "undefined"
          },
          {
            "id": 3,
            "title": "Optimize matching algorithms with spatial indexing",
            "description": "Implement scipy.spatial.KDTree for efficient spatial queries in anomaly matching",
            "dependencies": [
              1
            ],
            "details": "Replace linear search algorithms with KDTree-based spatial indexing using scipy.spatial.KDTree for efficient nearest neighbor queries. Implement optimized matching that builds KDTree from run1 positions and queries run2 positions with configurable k-nearest neighbors. Add support for distance-based filtering and batch query processing. Optimize tree construction and query parameters for different dataset sizes.",
            "status": "pending",
            "testStrategy": "Benchmark KDTree vs linear search performance on datasets of varying sizes. Verify matching accuracy and consistency. Test with different k-values and distance thresholds.",
            "parentId": "undefined"
          },
          {
            "id": 4,
            "title": "Add progress tracking with tqdm progress bars",
            "description": "Implement progress bars using tqdm for long-running operations to provide user feedback",
            "dependencies": [
              2,
              3
            ],
            "details": "Integrate tqdm progress bars for all time-consuming operations including data loading, chunked processing, spatial indexing, and matching algorithms. Add nested progress bars for multi-stage operations with descriptive labels. Implement progress estimation based on data size and historical performance metrics. Add configurable verbosity levels and silent mode for automated processing.",
            "status": "pending",
            "testStrategy": "Verify progress bar accuracy and smooth updates during processing. Test nested progress bars and different verbosity levels. Ensure no performance impact from progress tracking.",
            "parentId": "undefined"
          },
          {
            "id": 5,
            "title": "Implement parallel processing with multiprocessing.Pool",
            "description": "Add parallel processing capabilities for independent calculations using multiprocessing.Pool",
            "dependencies": [
              3,
              4
            ],
            "details": "Implement parallel processing using multiprocessing.Pool for embarrassingly parallel operations like growth rate calculations and anomaly pair processing. Add automatic CPU core detection and configurable worker count. Implement proper data serialization for inter-process communication and result aggregation. Add fallback to serial processing for small datasets where parallelization overhead exceeds benefits.",
            "status": "pending",
            "testStrategy": "Verify parallel processing produces identical results to serial processing. Benchmark performance scaling with different worker counts. Test memory usage and process management.",
            "parentId": "undefined"
          },
          {
            "id": 6,
            "title": "Implement multi-run anomaly tracking system",
            "description": "Build anomaly tracks across 3+ runs by assigning track_id to anomalies representing the same physical feature",
            "dependencies": [
              3,
              5
            ],
            "details": "Extend the matching system to handle 3+ runs (2007, 2015, 2022) by implementing a multi-run tracking algorithm. Create track assignment logic that uses spatial proximity and feature characteristics to link anomalies across multiple inspections. Implement track_id generation system that maintains consistency across runs while handling cases where features appear, disappear, or are not detected in certain runs. Use optimized spatial indexing for efficient multi-run matching.",
            "status": "pending",
            "testStrategy": "Test tracking with synthetic 3+ run datasets. Verify track continuity and proper handling of missing detections. Validate track_id uniqueness and consistency.",
            "parentId": "undefined"
          },
          {
            "id": 7,
            "title": "Create tracks_multi_run.csv output format",
            "description": "Generate comprehensive multi-run tracking output with track_id and run-specific measurements",
            "dependencies": [
              6
            ],
            "details": "Implement tracks_multi_run.csv output generation with track_id as primary key and columns for each run's feature_id and measurements (depth, length, width, clock_position, distance). Handle gaps gracefully where a feature was not detected in a specific run by using NaN or empty values. Include metadata columns for track confidence, first_detected_run, last_detected_run, and total_detections. Ensure output format supports easy analysis and visualization of anomaly evolution over time.",
            "status": "pending",
            "testStrategy": "Validate CSV output format with various multi-run scenarios. Test gap handling and metadata accuracy. Verify compatibility with downstream analysis tools.",
            "parentId": "undefined"
          },
          {
            "id": 8,
            "title": "Add --enable_multirun CLI flag and run list support",
            "description": "Implement command-line interface for multi-run processing with configurable run list",
            "dependencies": [
              7
            ],
            "details": "Add --enable_multirun CLI flag to activate multi-run tracking mode. Implement run list parameter that accepts multiple run identifiers (e.g., --runs 2007,2015,2022). Modify argument parsing to handle multi-run mode and validate run list format. Update help documentation and error handling for multi-run specific parameters. Ensure backward compatibility with existing two-run processing mode.",
            "status": "pending",
            "testStrategy": "Test CLI flag functionality with various run list formats. Verify backward compatibility with two-run mode. Test error handling for invalid run specifications.",
            "parentId": "undefined"
          }
        ]
      },
      {
        "id": 13,
        "title": "Implement Anomaly Clustering for Interaction Zones",
        "description": "Implement DBSCAN clustering on anomaly distances to group spatially close anomalies that may interact, adding cluster analysis and aggregated metrics to the pipeline output.",
        "details": "Implement anomaly clustering using DBSCAN from scikit-learn in a new src/clustering.py module. After anomaly matching is complete, extract anomaly positions as 1D distances along pipe or 2D coordinates (distance, clock_position) from matched_results.csv. Apply DBSCAN clustering with configurable epsilon parameter (default 50ft for 1D, or combined distance for 2D). Handle both coordinate systems: 1D clustering uses absolute distance differences, 2D clustering uses Euclidean distance in (distance, normalized_clock) space where clock positions are normalized to [0,1] range. Add cluster_id column to matched_results.csv output (-1 for noise/unclustered anomalies). Compute per-cluster aggregated metrics including: total_metal_loss_area (sum of individual anomaly areas), cluster_centroid_distance, cluster_span (max-min distance), average_depth, cluster_growth_rate (weighted average of individual growth rates), anomaly_count. Generate clusters_summary.csv with one row per cluster containing aggregated metrics. Add CLI parameter --clustering_epsilon for distance threshold and --clustering_mode for 1D/2D selection. Integrate clustering as optional post-processing step after matching pipeline completion.",
        "testStrategy": "Test DBSCAN clustering with synthetic anomaly data having known spatial clusters. Verify 1D and 2D clustering modes produce expected groupings. Test epsilon parameter sensitivity with various distance thresholds. Validate cluster_id assignment in matched_results.csv output. Test aggregated metrics calculations for accuracy (total area, growth rates, centroids). Verify handling of edge cases: no clusters found (all noise), single anomaly clusters, overlapping anomalies. Test integration with existing pipeline ensuring clustering runs after matching completion. Validate clusters_summary.csv output format and metric calculations.",
        "status": "pending",
        "dependencies": [
          5
        ],
        "priority": "medium",
        "subtasks": []
      },
      {
        "id": 14,
        "title": "Implement Corrosion Growth Forecasting",
        "description": "Implement corrosion growth forecasting for matched anomalies using linear extrapolation to project future depths and calculate time to critical thresholds.",
        "details": "Implement corrosion growth forecasting in src/forecasting.py module. For each matched anomaly, calculate projected_depth = depth_run2 + growth_rate * forecast_years using linear extrapolation. Calculate years_to_critical = (critical_depth - depth_run2) / growth_rate where critical_depth defaults to 80% wall thickness. Add columns projected_depth_5yr and years_to_80pct to matched_results.csv output. Add CLI parameter --forecast_years with default value of 5 years. When 3+ runs are available, implement growth acceleration detection by comparing growth rate intervals between consecutive run pairs. Flag anomalies where later growth rate significantly exceeds earlier rate (e.g., >50% increase) indicating potential acceleration. Handle edge cases: negative growth rates (set years_to_critical to infinity), growth rates of zero (set to infinity), and missing wall thickness data (use pipeline default or skip forecasting). Add acceleration_flag column to output indicating detected growth acceleration. Integrate with existing growth rate calculations from Task 6 and ensure compatibility with multi-run scenarios from Task 12.",
        "testStrategy": "Test linear extrapolation calculations with known growth rates and time intervals. Verify years_to_critical calculations with various critical depth thresholds. Test CLI parameter --forecast_years with different values. Test growth acceleration detection with synthetic 3-run data having known acceleration patterns. Verify handling of edge cases: negative growth, zero growth, missing wall thickness. Test output CSV format includes new forecasting columns. Validate forecasting accuracy by comparing predictions with actual historical data when available.",
        "status": "pending",
        "dependencies": [
          6
        ],
        "priority": "medium",
        "subtasks": []
      }
    ],
    "metadata": {
      "version": "1.0.0",
      "lastModified": "2026-02-07T20:50:50.835Z",
      "taskCount": 12,
      "completedCount": 2,
      "tags": [
        "master"
      ],
      "created": "2026-02-07T20:52:36.955Z",
      "description": "Tasks for master context",
      "updated": "2026-02-07T20:53:11.945Z"
    }
  }
}